from bs4 import BeautifulSoup
import json
import os

# --- C·∫§U H√åNH ---
# T√™n file b·∫°n v·ª´a l∆∞u t·ª´ tr√¨nh duy·ªát v·ªÅ
LOCAL_FILE = "banggia.html" 
BASE_URL = "https://vnexpress.net"

def crawl_from_local_file():
    print(f"üìÇ ƒêang m·ªü file '{LOCAL_FILE}' tr√™n m√°y c·ªßa b·∫°n...")
    
    # Ki·ªÉm tra file c√≥ t·ªìn t·∫°i kh√¥ng
    if not os.path.exists(LOCAL_FILE):
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file. B·∫°n ƒë√£ l∆∞u file 'banggia.html' v√†o c√πng th∆∞ m·ª•c ch·ª©a code ch∆∞a?")
        return

    # ƒê·ªçc file HTML
    with open(LOCAL_FILE, "r", encoding="utf-8") as f:
        html_content = f.read()
    
    soup = BeautifulSoup(html_content, "html.parser")
    
    # T√¨m c√°c d√≤ng trong b·∫£ng gi√° (class 'banggiaxe-item')
    # L∆∞u √Ω: Khi l∆∞u v·ªÅ m√°y, tr√¨nh duy·ªát c√≥ th·ªÉ render c·∫•u tr√∫c h∆°i kh√°c, ta t√¨m class ch√≠nh x√°c
    rows = soup.find_all("tr", class_="banggiaxe-item")
    
    if len(rows) == 0:
        print("‚ö†Ô∏è V·∫´n ch∆∞a t√¨m th·∫•y d√≤ng n√†o. H√£y th·ª≠ m·ªü file HTML l√™n xem b·∫£ng gi√° c√≥ trong ƒë√≥ kh√¥ng.")
        # Fallback: Th·ª≠ t√¨m theo class init-banggia (nh∆∞ b·∫°n cung c·∫•p)
        rows = soup.find_all("tr", class_="init-banggia")
        
    print(f"üìä T√¨m th·∫•y {len(rows)} d√≤ng xe trong file!")
    
    unique_vehicles = {} 
    
    for row in rows:
        cols = row.find_all("td")
        if len(cols) < 5: continue
        
        # L·∫•y th√¥ng tin c∆° b·∫£n
        # C·∫ßn try-catch v√¨ c·∫•u tr√∫c HTML l∆∞u v·ªÅ m√°y c√≥ th·ªÉ c√≥ r√°c
        try:
            brand = cols[0].text.strip()
            # T√¨m th·∫ª a trong c·ªôt th·ª© 2 (T√™n xe)
            link_tag = cols[1].find("a")
            model_name = link_tag.text.strip() if link_tag else cols[1].text.strip()
            
            version = cols[2].text.strip()
            v_type = cols[3].text.strip() # "Xe s·ªë", "Xe tay ga"...
            
            # L·∫•y Link chi ti·∫øt (QUAN TR·ªåNG)
            if link_tag:
                raw_link = link_tag.get("href")
                clean_link = raw_link.split("#")[0]
                if not clean_link.startswith("http"):
                    clean_link = BASE_URL + clean_link
                
                v_id = clean_link.split("/")[-1]
                
                # Logic Map Template (Gi·ªØ nguy√™n nh∆∞ c≈©)
                template_id = "under_350cc"
                tags = []
                if "tay ga" in v_type.lower(): tags.append("scooter")
                elif "s·ªë" in v_type.lower(): tags.append("manual")
                elif "c√¥n" in v_type.lower(): tags.append("manual"); tags.append("chain_drive")
                elif "ƒëi·ªán" in v_type.lower(): template_id = "electric"; tags.append("ev")

                # L∆∞u v√†o dict
                if v_id not in unique_vehicles:
                    unique_vehicles[v_id] = {
                        "id": v_id,
                        "name": model_name,
                        "brand": brand,
                        "type": v_type,
                        "template_id": template_id,
                        "tags": tags,
                        "detail_url": clean_link,
                        "versions": [version],
                        "specs": {} # S·∫Ω l·∫•y ·ªü b∆∞·ªõc sau (online)
                    }
                else:
                    if version not in unique_vehicles[v_id]["versions"]:
                        unique_vehicles[v_id]["versions"].append(version)
        except Exception as e:
            continue # B·ªè qua d√≤ng l·ªói
            
    # Xu·∫•t ra JSON
    if unique_vehicles:
        final_list = list(unique_vehicles.values())
        print(f"‚ú® T·ªïng h·ª£p ƒë∆∞·ª£c: {len(final_list)} d√≤ng xe duy nh·∫•t.")
        
        with open("vehicles_from_local.json", "w", encoding="utf-8") as f:
            json.dump(final_list, f, ensure_ascii=False, indent=2)
        print("‚úÖ ƒê√£ t·∫°o file 'vehicles_from_local.json'. H√£y m·ªü ra ki·ªÉm tra!")
    else:
        print("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu n√†o.")

if __name__ == "__main__":
    crawl_from_local_file()